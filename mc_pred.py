# -*- coding: utf-8 -*-
"""Copy of Multi-label Emotion Classification + W&B.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/arghyadeep99/Multi-label-Emotion-Classification-using-PyTorch-and-W-B/blob/main/Multi-label%20Emotion%20Classification%20with%20Pytorch%2C%20transformers%20and%20W%26B.ipynb
"""

import argparse
import os
import pickle as origin_pickle
import random

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import transformers
from datasets import load_dataset
from sklearn import metrics, model_selection, preprocessing
from torch.utils.data import DataLoader, Dataset
from tqdm.notebook import tqdm
from transformers import AdamW, get_linear_schedule_with_warmup

import befutils
import utils


def seed_everything(seed):
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True


def init_args():
    parser = argparse.ArgumentParser(description="")
    parser.add_argument(
        "--dev_set", default="data/train_dev/DEV.prep_feedback_comment.public.tsv"
    )
    parser.add_argument("--model_path")
    parser.add_argument("--out_path")
    parser.add_argument(
        "--model",
        default="roberta-large",
        choices=[
            "squeezebert/squeezebert-uncased",
            "bert-base-uncased",
            "roberta-base",
            "roberta-large",
        ],
    )
    parser.add_argument("--grammar_term_set", default="data/grammar_terms/grammar_terms.small.set")
    parser.add_argument(
        "--given_offset_phrase",
        action="store_true",
        help="Flag to give offset phrase to source.",
    )
    parser.add_argument("--is_test", action="store_true")
    parser.add_argument("--top_n", type=int, default=0)
    parser.add_argument("--exclude_grammar_terms_by_count", type=int, default=0)
    parser.add_argument("--exclude_preposition_tag", action="store_true")
    parser.add_argument("-b", "--batch_size", type=int, default=16)
    parser.add_argument("--epoch", type=int, default=5)
    parser.add_argument("-lr", "--learning_rate", type=float, default=4e-5)
    parser.add_argument("--gpu_id", type=int, default=0)
    parser.add_argument("--class_weight_pkl", default="data/grammar_terms_weight.pkl")
    parser.add_argument("--optimizer", default="adamw")
    parser.add_argument("--dont_save_model", action="store_true")
    parser.add_argument("--thresh", type=float, default=0)
    parser.add_argument("--without_idf", action="store_true")
    args = parser.parse_args()
    assert not (
        args.dev_set == "data/train_dev/DEV.prep_feedback_comment.public.tsv"
        and args.thresh == 0
    )
    return args


def dataset_to_df(
    dataset_path,
    gt2id,
    columns,
    exclude_grammar_terms_by_count,
    is_test=False,
    offset_phrase=False
):
    data_list = []
    with open(dataset_path, encoding="utf-8") as f:
        for line in f:
            if is_test:
                source, offset = line.strip().split("\t")
                target = "<preposition>" # this is not used
            else:
                source, offset, target = line.strip().split("\t")
            target = utils.clean_up_data(target)
            grammar_terms = utils.extract_grammar_terms(target)
            grammar_terms = utils.exclude_grammar_terms_by_count(
                grammar_terms, gt2id, 0
            )
            grammar_term_ids = []
            for term in grammar_terms:
                grammar_term_ids.append(gt2id[term])
            source = utils.add_info_to_source(
                source,
                offset=offset,
                offset_phrase=offset_phrase,
                add_prefix=False,
            )
            off_s, off_e = map(int, offset.split(":"))
            noff_s = source[:off_s].count(" ")
            noff_e = source[:off_e].count(" ") + 1
            new_data = [(noff_s, noff_e), source.lower()] + [grammar_term_ids]
            data_list.append(new_data)
    df = pd.DataFrame(data_list, columns=columns)
    return df

def one_hot_encoder(df, n_labels):
    one_hot_encoding = []
    for i in tqdm(range(len(df))):
        temp = [0] * n_labels
        label_indices = df.iloc[i]["labels"]
        for index in label_indices:
            temp[index] = 1
        one_hot_encoding.append(temp)
    return pd.DataFrame(one_hot_encoding)


class Dataset:
    def __init__(self, texts, labels, offsets, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.offsets = offsets

        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, index):
        text = self.texts[index]
        label = self.labels[index]
        offset = self.offsets[index]

        inputs = self.tokenizer.__call__(
            text,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            padding="max_length",
            truncation=True,
        )
        noff_s = -1
        noff_e = -1
        for idx, id in enumerate(inputs.word_ids()):
            if noff_s == -1 and id == offset[0]:
                noff_s = idx
            if noff_e == -1 and id == offset[1]:
                noff_e = idx
        noffset = [noff_s, noff_e]
        ids = inputs["input_ids"]
        mask = inputs["attention_mask"]

        return {
            "ids": torch.tensor(ids, dtype=torch.long),
            "mask": torch.tensor(mask, dtype=torch.long),
            "labels": torch.tensor(label, dtype=torch.long),
            "offsets": torch.tensor(noffset, dtype=torch.long),
        }


class Classifier(nn.Module):
    def __init__(self, n_train_steps, n_classes, do_prob, bert_model):
        super(Classifier, self).__init__()
        self.bert = bert_model
        self.dropout = nn.Dropout(do_prob)
        self.out = nn.Linear(1024, n_classes)
        self.n_train_steps = n_train_steps
        self.step_scheduler_after = "batch"

    def forward(self, ids, mask, offsets):
        hiddens = self.bert(ids, attention_mask=mask)["last_hidden_state"]
        output_1 = []
        for hidden, offset in zip(hiddens, offsets):
            output_1.append(torch.mean(hidden[offset[0] : offset[1]], 0))
        output_1 = torch.stack(output_1)
        output_2 = self.dropout(output_1)
        output = self.out(output_2)
        return output


def build_dataloader(valid_dataset, batch_size):
    valid_data_loader = DataLoader(
        valid_dataset, batch_size=batch_size, shuffle=True, num_workers=1
    )
    return valid_data_loader


def ret_model(do_prob, path):
    return model



def loss_fn(outputs, labels):
    if labels is None:
        return None
    return nn.BCEWithLogitsLoss()(outputs, labels.float())


def log_metrics(preds, labels):
    preds = torch.stack(preds)
    preds = preds.cpu().detach().numpy()
    labels = torch.stack(labels)
    labels = labels.cpu().detach().numpy()

    """
    auc_micro_list = []
    for i in range(n_labels):
      current_pred = preds.T[i]
      current_label = labels.T[i]
      fpr_micro, tpr_micro, _ = metrics.roc_curve(current_label.T, current_pred.T)
      auc_micro = metrics.auc(fpr_micro, tpr_micro)
      auc_micro_list.append(auc_micro)
    
    return {"auc": np.array(auc_micro).mean()}
    """

    fpr_micro, tpr_micro, thresholds = metrics.roc_curve(labels.ravel(), preds.ravel())

    auc_micro = metrics.auc(fpr_micro, tpr_micro)
    return auc_micro, thresholds


def train_fn(data_loader, model, optimizer, device, scheduler):
    """
    Modified from Abhishek Thakur's BERT example:
    https://github.com/abhishekkrthakur/bert-sentiment/blob/master/src/engine.py
    """

    train_loss = 0.0
    model.train()
    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):
        ids = d["ids"]
        mask = d["mask"]
        targets = d["labels"]

        ids = ids.to(device, dtype=torch.long)
        mask = mask.to(device, dtype=torch.long)
        targets = targets.to(device, dtype=torch.float)

        optimizer.zero_grad()
        outputs = model(ids=ids, mask=mask)

        loss = loss_fn(outputs, targets)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()
        scheduler.step()
    return train_loss


def eval_fn(data_loader, model, device, weight=None):
    """
    Modified from Abhishek Thakur's BERT example:
    https://github.com/abhishekkrthakur/bert-sentiment/blob/master/src/engine.py
    """
    eval_loss = 0.0
    model.eval()
    fin_targets = []
    fin_outputs = []
    with torch.no_grad():
        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):
            ids = d["ids"]
            mask = d["mask"]
            targets = d["labels"]
            offsets = d["offsets"]

            ids = ids.to(device, dtype=torch.long)
            mask = mask.to(device, dtype=torch.long)
            targets = targets.to(device, dtype=torch.float)

            outputs = model(ids=ids, mask=mask, offsets=offsets)
            outputs = outputs * torch.tensor(weight).to(device)
            loss = loss_fn(outputs, targets)
            eval_loss += loss.item()
            fin_targets.extend(targets)
            fin_outputs.extend(torch.sigmoid(outputs))
    return eval_loss, fin_outputs, fin_targets


def load_weight(class_weight_pkl):
    with open(class_weight_pkl, "rb") as f:
        weight = origin_pickle.load(f)
    return weight



def main():
    seed_everything(1234)

    args = init_args()

    gts = utils.read_label(
        args.grammar_term_set
    )
    if args.top_n != 0: # if set top_n
        gts = gts[:args.top_n]
    gt2id = {gt: i for i, gt in enumerate(gts)}

    columns = ["offset", "text", "labels"]
    valid = dataset_to_df(
        args.dev_set,
        gt2id,
        columns,
        args.exclude_grammar_terms_by_count,
        is_test=args.is_test,
        offset_phrase=args.given_offset_phrase
    )

    mapping = {value: key for key, value in gt2id.items()}  # id2grammar_term

    n_labels = len(mapping)

    valid_ohe_labels = one_hot_encoder(valid, n_labels)

    valid = pd.concat([valid, valid_ohe_labels], axis=1)


    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model, do_lower_case=True)

    bert_model = transformers.AutoModel.from_pretrained(args.model)


    max_len = 256
    test_dataset = Dataset(
        valid.text.tolist(),
        valid[range(n_labels)].values.tolist(),
        valid.offset.tolist(),
        tokenizer,
        max_len,
    )

    test_data_loader = build_dataloader(test_dataset, args.batch_size)
    print("Length of Valid Dataloader: ", len(test_dataset))

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = Classifier(1, n_labels, 0.1, bert_model=bert_model)
    model.to(device)
    model = nn.DataParallel(model)
    model.load_state_dict(torch.load(args.model_path))

    weight = load_weight(args.class_weight_pkl)
    if args.without_idf:
        weight = [1] * len(weight)
        
    eval_loss, preds, labels = eval_fn(test_data_loader, model, device, weight)

    auc_score, thresholds = log_metrics(preds, labels)
    print("AUC score: ", auc_score)

    if args.thresh != 0:
        thresholds = [args.thresh]

    # --- prediction ---

    label_num = len(test_dataset)
    best_num = -1
    best_threshold = 0
    for idx, threshold in enumerate(thresholds):
        x = torch.zeros_like(torch.stack(preds))
        x[torch.stack(preds) > threshold] = 1
        num = 0
        for j, lx in enumerate(x):
            if torch.equal(lx.int(), labels[j].int()):
                num += 1
        # num = torch.count_nonzero(x).item()
        if best_num == -1 or best_num > abs(num - label_num):
            best_num = abs(num - label_num)
            pred_labels_list = torch.nonzero(torch.stack(preds) > threshold)
            best_threshold = threshold

    print(f"best_num: {best_num}, label_num: {label_num}")

    prob_id = 0
    with open(args.out_path, "w", encoding="utf-8") as f:
        for i, pred_labels in enumerate(pred_labels_list):
            if i > 0:
                while prob_id != pred_labels[0].item():
                    print(file=f)
                    prob_id += 1
            print(mapping[pred_labels[1].item()], end="\t", file=f)

    with open(args.out_path + ".best_threshold", "w", encoding="utf-8") as f:
        print(best_threshold, file=f)

if __name__ == "__main__":
    main()